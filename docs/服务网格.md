### 服务网格
> 我们使用istio服务网格来实现我们的微服务治理， istio是一种跟平台和语言无关的服务治理方案

<img align="center" width="600px" src="../assets/docs-arch.svg">

> 需要了解更多istio知识可以查看https://istio.io/latest/docs/ops/deployment/architecture/

### 安装K3S

1. 先下载镜像， 加快部署（github太慢， QAQ）  

```shell
export chifun=https://github.91chifun.workers.dev
wget ${chifun}/https://github.com//k3s-io/k3s/releases/download/v1.21.2%2Bk3s1/k3s-airgap-images-amd64.tar  
wget ${chifun}/https://github.com//k3s-io/k3s/releases/download/v1.21.2%2Bk3s1/k3s  
```

2. 本地安装k3s

```shell
sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp ./k3s /usr/local/bin/
sudo cp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/
sudo chmod +x /usr/local/bin/k3s
curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC="--disable traefik" \
  K3S_KUBECONFIG_MODE=644 sh -
```

3. 测试是否安装成功
   
```shell
[double@double nfs]$ sudo kubectl top pod --all-namespaces --use-protocol-buffers 
NAMESPACE     NAME                                      CPU(cores)   MEMORY(bytes)   
kube-system   coredns-7448499f4d-d4gkz                  5m           14Mi            
kube-system   local-path-provisioner-5ff76fc89d-v9kpq   3m           8Mi             
kube-system   metrics-server-86cbb8457f-sw47l           2m           13Mi             
```

    这里只是本地的k3s, 假如需要HA, 可以更新到最新k3s, 以及多节点管理， 这里不做多演示。

### 安装存储

    这里我们使用nfs的存储中介， 可以根据实际应用做调整

1. 开启内核模块 nfs, nfsd, rpcsec_gss_krb5 (only if Kerberos is used)

```shell
sudo modprobe {nfs,nfsd,rpcsec_gss_krb5}
```

2. 运行容器

```shell
docker run                                                       \
  -v /home/double/App/docker/nfs/data:/var/local/nfs/data        \
  -e NFS_EXPORT_0='/var/local/nfs/data  *(ro,no_subtree_check)'  \
  --cap-add SYS_ADMIN                                 \
  -p 2049:2049   -p 2049:2049/udp                     \
  -p 111:111     -p 111:111/udp                       \
  -p 32765:32765 -p 32765:32765/udp                   \
  -p 32767:32767 -p 32767:32767/udp                   \
  -d erichough/nfs-server
```

3. 安装storageclass

> 创建dolphin命名空间

```shell
sudo kubectl create namespace dolphin
```

> 创建部署文件（根据上面docker的配置修改下面的 NFS_SERVER， NFS_PATH）

deployment.yaml

```yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-client-provisioner
  namespace: dolphin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: dolphin/nfs
            - name: NFS_SERVER
              value: 172.16.10.191
            - name: NFS_PATH
              value: /var/local/nfs/data
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.16.10.191
            path: /var/local/nfs/data
```

rbac.yaml

```yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: nfs-client-provisioner
  namespace: dolphin
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
  namespace: dolphin
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
  namespace: dolphin
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: dolphin
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: dolphin
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: dolphin
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: dolphin
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```

storageclass.yaml

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
  namespace: dolphin
provisioner: dolphin/nfs # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  archiveOnDelete: "false"
```

> 部署服务

```shell
sudo kubectl apply -f ./deployment.yaml ./rbac.yaml ./storageclass.yaml
```

```shell
[double@double nfs]$ sudo kubectl get all -n dolphin
NAME                                         READY   STATUS              RESTARTS   AGE
pod/nfs-client-provisioner-f84497cc7-4vb45   0/1     ContainerCreating   0          63s

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nfs-client-provisioner   0/1     1            0           63s

NAME                                               DESIRED   CURRENT   READY   AGE
replicaset.apps/nfs-client-provisioner-f84497cc7   1         1         0       63s
```


### 安装ISTIO

1. 先下载镜像

```shell
export chifun=https://github.91chifun.workers.dev
wget ${chifun}/https://github.com//istio/istio/releases/download/1.10.2/istio-1.10.2-linux-amd64.tar.gz
```

2. 安装istio

```shell
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml 
tar zxvf istio-1.10.2-linux-amd64.tar.gz && cd istio-1.10.2 
sudo cp bin/istioctl /usr/local/bin && istioctl install --set profile=demo -y
```

3. 安装bookinfo

```shell
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
```


### 安装Rancher

```shell
sudo docker run --privileged -v /home/double/App/docker/rancher/data:/var/lib/rancher -p 8888:80 \
  -p 8443:443 -d rancher/rancher
```
